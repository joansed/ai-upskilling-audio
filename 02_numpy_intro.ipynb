{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682693e8",
   "metadata": {},
   "source": [
    "# Fasterer Python\n",
    "\n",
    "Pure Python is slow, for two reasons. First, it is an **interpreted language**, which is always less efficient than a compiled language. Second, Python was originally conceived to work on a single thread, to make programming easier. It has a **Global Interpreter Lock (GIL)**, which means that only one python instruction is carried out at a time.\n",
    "\n",
    "However, there are easy ways to make Python code run **very fast**, circumventing these problems.\n",
    "\n",
    "First, Python is very good at interfacing with non-Python libraries. This means you can write fast, compiled code in Rust or C++, and then call it from Python. The compiled libraries are exempt from the GIL, meaning that they can do their own threading, as well.\n",
    "\n",
    "Second, there are a couple forms of parallelism within Python which circumvent the GIL.\n",
    "\n",
    "## Numpy\n",
    "\n",
    "Numpy (\"Numerical Python\") is a fast library for doing all kinds of numerical array operations in Python. It's written in an unholy combination of C++ and Fortran, but as a user, you never have to worry about that!\n",
    "\n",
    "The golden rule of Numpy is that using numpy operations on numpy arrays is going to be fast, but using Python operations will be slow. For example, iterating over a matrix with for loops is slow, but operating on a matrix with numpy operations is fast.\n",
    "\n",
    "Let's see an example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f03dc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# ðŸ“ Exercise 0 â€“ Pure Python vs Numpy\n",
    "# ------------------------------------------------------------\n",
    "# First, let's create a Numpy matrix and perform some\n",
    "# operation to confirm that, indeed, numpy is faster than\n",
    "# pure python. Mostly we just want to get used to using basic\n",
    "# numpy operations.\n",
    "#\n",
    "# 1) Create a 1024x1024 numpy array, which is 1 along the main\n",
    "#    diagonal, and 0.1 everywhere else.\n",
    "# 2) Write a function which adds a constant to each entry in\n",
    "#    the matrix, using Python for loops to update each entry.\n",
    "# 3) Write the same function using a one-line numpy operation.\n",
    "# 4) Use `%time` to compare the running time of the two\n",
    "#    functions, when run on the matrix from step (1).\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Make a matrix of random numbers.\n",
    "mat = 1 # Turn this into a 1024x1024 matrix, as specified above.\n",
    "\n",
    "def add_constant_python(mat, constant):\n",
    "    raise NotImplementedError(\"Add your code here\")\n",
    "\n",
    "def add_constant_numpy(mat, constant):\n",
    "    raise NotImplementedError(\"Add your code here\")\n",
    "\n",
    "# Test the functions\n",
    "%time add_constant_python(mat, 1)\n",
    "%time add_constant_numpy(mat, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed79b8",
   "metadata": {},
   "source": [
    "## Building a \"Tokenizer\" (or, k-means clustering for the young and fancy.)\n",
    "\n",
    "Now let's do something a bit more meaty...\n",
    "\n",
    "Lately, working with tokenized vectors is popular - in short, you take some inputs, feed them through a few layers of neural network and turn them into vectors. And then you turn the vectors into discrete objects. How does that work? Clustering! You assign vectors to clusters, and just return the number of the cluster each vector was assigned to.\n",
    "\n",
    "There are approximately a thousand ways to do clustering, but it turns out that k-means is still surprisingly useful, and more-or-less what people do in practice. So, let's do some k-means clustering.\n",
    "\n",
    "To do k-means clustering, you need some centroids.  \n",
    "Then you alternate two steps: \n",
    "1) Assign data points to centroids. \n",
    "2) Update the centroid as the mean of all the data points assigned to it.\n",
    "\n",
    "Finally, it's somewhat helpful to keep track of how far each data point is from its assigned centroid - this is the \"loss\".\n",
    "\n",
    "Let's implement all of these operations with numpy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8c81ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c362ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# ðŸ“ Exercise 1 â€“ Tokenizer\n",
    "# ------------------------------------------------------------\n",
    "# To build our tokenizer/k-means clusterer, we'll need three operations:\n",
    "# a) Generate initial centroids.\n",
    "# b) Assign data points to clusters.\n",
    "# c) Update the centroids from the data.\n",
    "# d) Compute the total distance from the data to the assigned cnetroids.\n",
    "# \n",
    "# Write a function for each of these, following the documentation.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def generate_initial_centroids(data: np.ndarray, k: int) -> np.ndarray:\n",
    "  \"\"\"Generate initial centroids for k-means clustering.\n",
    "\n",
    "  Par: 1 line of code.\n",
    "\n",
    "  Args:\n",
    "    data: The data to cluster.\n",
    "    k: The number of clusters to generate.\n",
    "  \n",
    "  Returns:\n",
    "    The initial centroids, an array with shape [k, data.shape[1]].\n",
    "  \"\"\"\n",
    "  raise NotImplementedError\n",
    "\n",
    "\n",
    "def assign_to_clusters(data: np.ndarray, centroids: np.ndarray) -> np.ndarray:\n",
    "  \"\"\"Assign data points to clusters.\n",
    "\n",
    "  Par: 2 lines of code.\n",
    "\n",
    "  Args:\n",
    "    data: The data to cluster.\n",
    "    centroids: The centroids to use for clustering.\n",
    "  \n",
    "  Returns:\n",
    "    Array with shape [data.shape[0],] of cluster assigments.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError\n",
    "\n",
    "\n",
    "def update_centroids(data: np.ndarray, centroids: np.ndarray) -> np.ndarray:\n",
    "  \"\"\"Update the centroids to be the mean of the data points in each cluster.\n",
    "\n",
    "  Par: 4 lines of code.\n",
    "\n",
    "  Args:\n",
    "    data: The data to cluster.\n",
    "    centroids: The centroids to use for clustering.\n",
    "  \n",
    "  Returns:\n",
    "    The updated centroids.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError\n",
    "\n",
    "def reconstruction_loss(data: np.ndarray, centroids: np.ndarray) -> float:\n",
    "  \"\"\"Compute the mean distance to the closest centroid.\n",
    "  \n",
    "  Par: 2 lines of code.\n",
    "\n",
    "  Args:\n",
    "    data: The data to cluster.\n",
    "    centroids: The centroids to use for clustering.\n",
    "  \n",
    "  Returns:\n",
    "    The mean distance to the closest centroid.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c26d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# ðŸ“ Exercise 1.5 â€“ Tokenizer\n",
    "# ------------------------------------------------------------\n",
    "# Now let's generate some data and train the tokenizer.\n",
    "# Write a loop which continuously improves the centroids.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Generate a random matrix of 1024 2-dimensional points.\n",
    "# data = ...\n",
    "\n",
    "# Get the initial centroids.\n",
    "k = 5\n",
    "centroids = generate_initial_centroids(data, k)\n",
    "\n",
    "# 1) Write a /training loop/ - each step, update the centroids and measure\n",
    "# the reconstruction loss. (100 steps should be plenty.)\n",
    "# 2) Store the reconstruction loss at each step in an array.\n",
    "# 3) Use plt.plot to plot the reconstruction loss.\n",
    "# 4) Use plt.scatter to plot the data and the centroids. Make it look nice.\n",
    "#    (Hint: 'alpha' can be used to make things transparent, 'c' can be used to \n",
    "#    to set a color for each point. The 'marker' and 'size' options might also\n",
    "#    be helpful...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c3ea1b",
   "metadata": {},
   "source": [
    "Extra challenges...\n",
    "\n",
    "1) Usually, you can't fit all the data in memory. In that case, you want to do *partial updates* of the cluster centroids. Change the `update_centroids` to take a step-size parameter. Replace each centroid with a weighted average of the current centroid and the mean of the points assigned to the centroid.\n",
    "\n",
    "2) A problem for k-means clustering on real data is 'dead' centroids. Come up with a rule to replace centroids which haven't been used in a while with a 'fresh' centroid.\n",
    "\n",
    "3) (Advanced!) When you have high-dimensional data, you need more centroids to represent the data. When you have lots of centroids, things slow down. What to do?? In *product quantization*, you break a vector into parts and apply k-means to each subvector. Then for any vector, it gets assigned a cluster for each sub-vector. Create a collection of centroids with shape `[K, P, N/P]`, and update your algorithm to train centroids for all of the sub-vectors. Try to stick to Numpy operations as much as possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c0923",
   "metadata": {},
   "source": [
    "## Randomness in numpy\n",
    "\n",
    "To get the hang of using generators and random numbers in numpy, let's make a synthetic data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aff3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# ðŸ“ Exercise 2 â€“ Random data generator\n",
    "# ------------------------------------------------------------\n",
    "# Write a generator which produces batches of random 2D points.\n",
    "# ------------------------------------------------------------\n",
    "from typing import Generator\n",
    "\n",
    "def random_gaussian_data_generator(\n",
    "    batch_size: int, \n",
    "    mu: np.ndarray,\n",
    "    sigma: np.ndarray) -> Generator[np.ndarray, None, None]:\n",
    "  \"\"\"Generate random data from a Gaussian distribution.\n",
    "\n",
    "  Args:\n",
    "    batch_size: The number of points to generate.\n",
    "    mu: The mean of the distribution with shape [D].\n",
    "    sigma: The standard deviation of the distribution with shape [D].\n",
    "  \n",
    "  Yields:\n",
    "    A batch of random data points.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df06edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# ðŸ“ Exercise 2 Challenge â€“ Random data generator\n",
    "# ------------------------------------------------------------\n",
    "# Write a generator which produces batches of random 2D points\n",
    "# from a mixture of Gaussians distribution.\n",
    "# The mixture of Gaussians is a weighted sum of multiple\n",
    "# Gaussian distributions. It is parameterized by a list of means,\n",
    "# a list of standard deviations, and a list of weights pi which\n",
    "# sum to 1.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def random_mixture_of_gaussians_data_generator(\n",
    "    batch_size: int, \n",
    "    mus: np.ndarray,\n",
    "    sigmas: np.ndarray,\n",
    "    pis: np.ndarray) -> Generator[np.ndarray, None, None]:\n",
    "  \"\"\"Generate random data from a mixture of Gaussians distribution.\n",
    "\n",
    "  Args:\n",
    "    batch_size: The number of points to generate.\n",
    "    mus: The means of the distributions with shape [K, D].\n",
    "    sigmas: The standard deviations of the distributions with shape [K, D].\n",
    "    pis: The weights of the distributions with shape [K].\n",
    "  \n",
    "  Yields:\n",
    "    A batch of random data points.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
